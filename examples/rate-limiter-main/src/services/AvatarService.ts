// Avatar service for integrating HuggingFace talking head models
export interface AvatarConfig {
  model: 'hallo' | 'sadtalker' | 'wav2lip' | 'musetalk';
  imageUrl?: string;
  voiceSettings?: {
    voice: string;
    speed: number;
    pitch: number;
  };
}

export interface AvatarResponse {
  videoUrl: string;
  audioUrl: string;
  duration: number;
  status: 'processing' | 'completed' | 'error';
}

class AvatarService {
  private readonly HF_API_BASE = 'https://api-inference.huggingface.co/models';
  private readonly HF_TOKEN = process.env.NEXT_PUBLIC_HF_TOKEN;

  // HuggingFace Spaces endpoints
  private readonly SPACES = {
    hallo: 'https://fffiloni-hallo-api.hf.space',
    sadtalker: 'https://sadtalker.hf.space',
    talkingFace: 'https://cvpr-ml-talking-face.hf.space'
  };

  async generateTalkingAvatar(
    text: string, 
    config: AvatarConfig = { model: 'hallo' },
    audioBlob?: Blob
  ): Promise<AvatarResponse> {
    try {
      console.log('üé≠ Generating talking avatar...');
      
      if (config.model === 'hallo') {
        return await this.generateWithHallo(text, config, audioBlob);
      } else if (config.model === 'sadtalker') {
        return await this.generateWithSadTalker(text, config, audioBlob);
      } else {
        throw new Error(`Model ${config.model} not implemented yet`);
      }
    } catch (error) {
      console.error('‚ùå Avatar generation failed:', error);
      throw error;
    }
  }

  private async generateWithHallo(
    text: string, 
    config: AvatarConfig,
    audioBlob?: Blob
  ): Promise<AvatarResponse> {
    // Step 1: Generate audio if not provided
    let audio = audioBlob;
    if (!audio) {
      audio = await this.generateTTS(text, config.voiceSettings);
    }

    // Step 2: Generate talking head video
    const formData = new FormData();
    formData.append('audio', audio, 'speech.wav');
    
    if (config.imageUrl) {
      // Download and append image
      const imageResponse = await fetch(config.imageUrl);
      const imageBlob = await imageResponse.blob();
      formData.append('image', imageBlob, 'avatar.jpg');
    }

    const response = await fetch(`${this.SPACES.hallo}/generate`, {
      method: 'POST',
      body: formData,
      headers: {
        'Authorization': `Bearer ${this.HF_TOKEN}`
      }
    });

    if (!response.ok) {
      throw new Error(`Hallo API error: ${response.statusText}`);
    }

    const result = await response.json();
    
    return {
      videoUrl: result.video_url,
      audioUrl: result.audio_url || URL.createObjectURL(audio),
      duration: result.duration || 10,
      status: 'completed'
    };
  }

  private async generateWithSadTalker(
    text: string,
    config: AvatarConfig,
    audioBlob?: Blob
  ): Promise<AvatarResponse> {
    // Similar implementation for SadTalker
    console.log('üé≠ Using SadTalker model...');
    
    // Step 1: Generate audio if needed
    let audio = audioBlob;
    if (!audio) {
      audio = await this.generateTTS(text, config.voiceSettings);
    }

    // SadTalker API call would go here
    // For now, return a mock response
    return {
      videoUrl: 'https://example.com/video.mp4',
      audioUrl: URL.createObjectURL(audio),
      duration: 10,
      status: 'completed'
    };
  }

  private async generateTTS(
    text: string, 
    voiceSettings?: AvatarConfig['voiceSettings']
  ): Promise<Blob> {
    console.log('üó£Ô∏è Generating speech audio...');
    
    // Use HuggingFace TTS models
    const response = await fetch(
      `${this.HF_API_BASE}/microsoft/speecht5_tts`,
      {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.HF_TOKEN}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          inputs: text,
          parameters: {
            speaker_embeddings: "default",
            ...voiceSettings
          }
        })
      }
    );

    if (!response.ok) {
      throw new Error(`TTS API error: ${response.statusText}`);
    }

    return await response.blob();
  }

  // Real-time streaming for conversation
  async startConversationStream(
    onAvatarUpdate: (response: AvatarResponse) => void,
    config: AvatarConfig = { model: 'hallo' }
  ): Promise<MediaStream> {
    console.log('üé™ Starting real-time avatar conversation...');
    
    // This would integrate with MuseTalk for real-time performance
    // Return a mock MediaStream for now
    return new MediaStream();
  }

  // Upload custom avatar image
  async uploadAvatarImage(file: File): Promise<string> {
    console.log('üì∏ Uploading avatar image...');
    
    const formData = new FormData();
    formData.append('image', file);
    
    // Upload to a service and return URL
    // For now, create object URL
    return URL.createObjectURL(file);
  }

  // Get available voices for TTS
  async getAvailableVoices(): Promise<Array<{id: string, name: string, language: string}>> {
    return [
      { id: 'alloy', name: 'Alloy', language: 'en' },
      { id: 'echo', name: 'Echo', language: 'en' },
      { id: 'fable', name: 'Fable', language: 'en' },
      { id: 'onyx', name: 'Onyx', language: 'en' },
      { id: 'nova', name: 'Nova', language: 'en' },
      { id: 'shimmer', name: 'Shimmer', language: 'en' }
    ];
  }
}

export const avatarService = new AvatarService(); 